# ğŸ¤– Chat IA avec Ollama

Application de chat avec intelligence artificielle locale utilisant Spring Boot et Angular.

![Spring Boot](https://img.shields.io/badge/Spring%20Boot-3.4.0-green)
![Angular](https://img.shields.io/badge/Angular-17-red)
![Java](https://img.shields.io/badge/Java-17-orange)

## ğŸ“¸ AperÃ§u

Chat moderne avec un modÃ¨le d'IA local (Llama 2, Mistral, etc.) qui sauvegarde automatiquement l'historique des conversations.

![Interface Chat](pictures/image1.png)


![Interface Chat](pictures/image2.png)


![Interface Chat](pictures/image3.png)



## âœ¨ FonctionnalitÃ©s

- ğŸ’¬ Chat en temps rÃ©el avec IA
- ğŸ“š Historique des conversations
- ğŸ¨ Interface moderne et responsive
- ğŸ›ï¸ Choix du modÃ¨le et de la tempÃ©rature
- âš¡ Affichage du temps de rÃ©ponse

## ğŸš€ Installation Rapide

### 1. Installer Ollama

**Windows:**
```bash
winget install Ollama.Ollama
```

**Mac:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. TÃ©lÃ©charger un modÃ¨le
```bash
ollama serve
ollama pull llama3.2
```

### 3. Backend Spring Boot
```bash
cd bdcc-ai-app
mvn spring-boot:run
```
Le backend dÃ©marre sur **http://localhost:8080**

### 4. Frontend Angular
```bash
cd bdcc-chat-angular
npm install
ng serve
```
Le frontend s'ouvre sur **http://localhost:4200**

## ğŸ“ Structure du Projet

```
bdcc-ai-app/                    # Backend Spring Boot
â”œâ”€â”€ src/main/java/
â”‚   â”œâ”€â”€ config/                 # Configuration Ollama
â”‚   â”œâ”€â”€ controller/             # API REST
â”‚   â”œâ”€â”€ service/                # Logique mÃ©tier
â”‚   â”œâ”€â”€ entity/                 # EntitÃ©s JPA
â”‚   â””â”€â”€ repository/             # AccÃ¨s base de donnÃ©es
â””â”€â”€ pom.xml

bdcc-chat-angular/              # Frontend Angular
â”œâ”€â”€ src/app/
â”‚   â”œâ”€â”€ models/                 # Interfaces TypeScript
â”‚   â”œâ”€â”€ services/               # Services HTTP
â”‚   â”œâ”€â”€ app.component.ts        # Composant principal
â”‚   â””â”€â”€ app.component.html      # Template
â””â”€â”€ package.json
```

## ğŸ¯ Utilisation

1. DÃ©marrez Ollama: `ollama serve`
2. Lancez le backend: `mvn spring-boot:run`
3. Lancez le frontend: `ng serve`
4. Ouvrez http://localhost:4200
5. Commencez Ã  chatter !

## ğŸ”Œ API Backend

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| GET | `/api/chat?message={text}` | Envoi simple |
| POST | `/api/chat` | Envoi avec options |
| GET | `/api/history` | Voir l'historique |
| DELETE | `/api/history` | Effacer l'historique |
| GET | `/api/health` | Ã‰tat de l'API |

### Exemple POST
```json
{
  "message": "Bonjour",
  "model": "llama3.2",
  "temperature": 0.7
}
```

## âš™ï¸ Configuration

### Backend (`application.properties`)
```properties
# Ollama
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=llama2

# Base de donnÃ©es
spring.datasource.url=jdbc:h2:mem:ollama_db
spring.h2.console.enabled=true

# CORS
spring.web.cors.allowed-origins=http://localhost:4200
```

### Changer le modÃ¨le
Dans `app.component.ts`:
```typescript
availableModels = [
  { value: 'llama3.2', label: 'Llama 3.2' },
  { value: 'quen3', label: 'Quen 3' },
  { value: 'gamma', label: 'Gamma' }
];
```

## ğŸ› ProblÃ¨mes FrÃ©quents

**Ollama ne rÃ©pond pas:**
```bash
ollama serve
```

**ModÃ¨le introuvable:**
```bash
ollama pull llama3.2
```

**Erreur CORS:**
VÃ©rifiez `spring.web.cors.allowed-origins` dans `application.properties`

**Backend ne dÃ©marre pas:**
```bash
mvn clean install
```

## ğŸ› ï¸ Technologies

**Backend:**
- Spring Boot 3.4.0
- Spring AI 1.0.0-M5
- H2 Database
- Lombok

**Frontend:**
- Angular 17
- TypeScript
- RxJS

**IA:**
- Ollama
- Llama 3.2 / Quen 3

## ğŸ“¦ Build Production

**Backend:**
```bash
mvn clean package
java -jar target/bdcc-ai-app-0.0.1-SNAPSHOT.jar
```

**Frontend:**
```bash
ng build --production
```

## ğŸ‘¨â€ğŸ’» Auteur

**Saif Dine HD**
- GitHub: [@votre-username](https://github.com/votre-username)

## ğŸ“ Licence

MIT License

## ğŸ™ Ressources

- [Spring AI](https://docs.spring.io/spring-ai/reference/)
- [Ollama](https://ollama.ai/)
- [Angular](https://angular.io/)

---

â­ N'oubliez pas de mettre une Ã©toile si ce projet vous aide !
